---
title: "HW 3"
author: "Riley Richardson"
date: "11/27/2023"
output: 
  html_document:
    number_sections: true
---

# 

In this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(tidyverse)
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
#plot(x, col=y)

ggplot(dat, aes(x.1, x.2, col = y)) + # Easier for me to see :)
  geom_point()+
  theme_minimal()

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
set.seed(1)

### Creating Training & Testing Sets
part_index <- sample(1:nrow(dat), 0.5*nrow(dat)) # 50/50 parition

train <- dat[part_index,] # Training Set
test <- dat[-part_index,] # Testing Set

# Support Vector Machine
svmfit <- svm(y~., data = train, kernel = "radial", gamma = 1, cost = 1, scale = F)
print(svmfit)

#grid
make.grid = function(x, n = 75) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(x.1 = x1, x.2 = x2)
}
xgrid = make.grid(x)

#overlaying prediction on the grid
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)

```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
# Support Vector Machine
svmfit2 <- svm(y~., data = train, kernel = "radial", gamma = 1, cost = 10000, scale = F)
print(svmfit2)

#overlaying prediction on the grid
ygrid = predict(svmfit2, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit2$index,], pch = 5, cex = 2)
```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

*It's possible if not likely that this SVM is overfitted to the data and not generalizable beyond the training data*

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r, eval = True}
# Confusion Matrix
table(true=dat[-part_index,"y"], pred=predict(svmfit2, newdata=dat[-part_index,]))
```

*The confusion matrix indicates that this model misclassifies 17 "1s" and 3 "2s." Additionally, it seems that there are far more "1s" than "2" in the testing dataset*

##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
# Training group composition
train %>% 
  group_by(y) %>%
  summarize(n=n())

# Testing group composition
test %>% 
  group_by(y) %>%
  summarize(n=n())

```

*The training partition has a slightly greater proportion of 2s than the testing data, but not absurdly so.*

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}

set.seed(1)

# Fine tuning the support vector machine
tune.out <- tune(svm, y~., data = train, 
     ranges = list(gamma = c(0.1, 1, 10, 100, 1000), 
                   cost = c(0.5, 1,2,3,4)),
     )

```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r, eval = FALSE}
# Confusion Matrix
table(true=dat[-part_index,"y"], pred=predict(tune.out$best.model, newdata=dat[-part_index,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

*The accuracy of this model is an improvement from the overfitted one with 91% accuracy, but it still misclassified nine "1s" as "2s." The model would likely be made better with the inclusion of more data. *

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}
# Binary factor variable
heart <- heart %>% mutate(
  class = ifelse(class > 0, "1", "0") %>% as.factor()
)

# Checking levels
heart$class %>% levels()

```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)
library(rpart)
library(rpart.plot)

# Partition into training & testing
train <- sample(1:nrow(heart), 240)

# Create decision tree
heart.tree <- tree(class~. - class, data = heart[train,], method = "class")
par(xpd = NA)

# Plot
plot(heart.tree)
text(heart.tree, pretty = 0)

```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
# Predictions & Confusion Matrix
tree.pred = predict(heart.tree, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, class))

(8+3)/(28+8+3+18)
```

*The classification error rate is 19%*

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
# Cross validation
cv.heart <- cv.tree(heart.tree, FUN = prune.misclass)

plot(cv.heart$size, cv.heart$dev, type = "b")

# Pruning
prune.heart = prune.misclass(heart.tree, best = 4)

# Plot
plot(prune.heart)
text(prune.heart, pretty = 0)

# Predictions & Confusion Matrix
tree.pred = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, class))

(10+4)/(26+4+10+17)

```

*The classification error rate is 25%*

##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

*By pruning the tree, our classification error rate increases by 6%, which is substantial. However, the product is an easily understandable tree, whereas that which produced an error of only 19% was virtually incomprehensible. Whether the sacrifice of accuracy for interpretability is "worth it" really depends on the application of the model.*

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

*One of the simplest ways a decision tree could exhibit algorithmic bias is if the training data are not representative of the general population. This would result in the algorithm giving undue weight to characteristics present only in the training data. Additionally, a non-representative training set would impact pruning --- you might get rid of a valuable node. This is perhaps included in the argument of a non-representative sample, but another way such a model could exhibit bias is through overfitting --- a "bushy" tree will likely perform well on training data, but likely will not be broadly generalizable.*







